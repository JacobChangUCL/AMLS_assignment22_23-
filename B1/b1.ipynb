{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as function\n",
    "from torchvision import datasets,transforms\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available()==False:\n",
    "    raise ImportError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_width=500\n",
    "picture_height=500 #the width and height of picture in cartoon\n",
    "#待会儿改成可以除以4的，比如54*4=216\n",
    "classesNum=5 #male or female\n",
    "epochs=2 #how much epoch I need to run\n",
    "batch_size0 =10 #I want to train it with minibatch.\n",
    "#there are 5000 images in celeba dataset\n",
    "img_path=os.getcwd()[:-2]+\"Datasets\\\\cartoon_set\" #[:-2]for deleting \"B1\"\n",
    "\n",
    "feature_size=4\n",
    "#I will double the feature_size after every pooling layer.so if you set feature_size to a very big number,\n",
    "#It will become very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#this class is for loading data.\n",
    "class DatasetCreater(Dataset):\n",
    "    def __init__(self,path:str,start_item:int,data_size:int,data_format:str,transformer:object):\n",
    "        #transformer is an object created by torchvision.transforms.Compose()to transform an picture from PIL object to tensor\n",
    "        super().__init__()\n",
    "        self.path=path\n",
    "        self.start_item=start_item\n",
    "        self.data_size=data_size\n",
    "        self.transformer=transformer\n",
    "        self.data_format=data_format\n",
    "            \n",
    "        #we process labels.Labels are small so I want to load every labes in the initial process\n",
    "        #pictures are big so I want to load it when I need to use it\n",
    "        labelPath=self.path+'\\\\labels.csv'\n",
    "        self.labels=pd.read_csv(labelPath)\n",
    "        self.labelsNew=[]\n",
    "        \n",
    "        for row in self.labels.iterrows():\n",
    "        #self.labels is a Dataframe,we can only use .iterrows()method to iter a dataframe\n",
    "            self.labelsNew.append(int(str(row[1]).split('\\\\t')[5]))\n",
    "        print(self.labelsNew.__len__()) \n",
    "        #[0,1,2,3,4]represents 5 kinds of eye colour\n",
    "        \n",
    "        #for i in range(0,self)\n",
    "        #现在我有一个imgs的数组，这个数组里面每一个元素都是一个图片tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    #哈哈，枚举类型和for循环也不知道可迭代对象有几个元素。他们就是通过__len__()函数找要迭代多少次的。如果__len__函数大于\n",
    "    #真实可迭代次数，就会报错\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #the path of one data\n",
    "        OneDataPath=self.path+\"\\\\img\\\\\"+str(item+self.start_item)+\".\"+self.data_format\n",
    "        image=Image.open(OneDataPath).convert('RGB')\n",
    "        imageTensor=self.transformer(image)\n",
    "\n",
    "        label=self.labelsNew[item+self.start_item]\n",
    "        return imageTensor,label\n",
    "\n",
    "transformer_train = transforms.Compose(\n",
    "    [\n",
    "   # transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.1),#随机水平翻转 选择一个概率概率 randon horizontal Flip\n",
    "    # transforms.RandomVerticalFlip(p=0.2),#随机垂直翻转 I have tested this function,the performance is not good.\n",
    "    #Because there is no need to identify head-down photos in the dataset and the tasks we need to complete\n",
    "    #transforms.RandomRotation(30),#随机旋转，-45到45度之间随机选\n",
    "    transforms.CenterCrop(400),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])         \n",
    "])\n",
    "\n",
    "transformer_test = transforms.Compose(\n",
    "    [\n",
    "   # transforms.Resize((256,256)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.2),#随机水平翻转 选择一个概率概率 randon horizontal Flip\n",
    "    # transforms.RandomVerticalFlip(p=0.2),#随机垂直翻转\n",
    "    transforms.CenterCrop(400),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])         \n",
    "])\n",
    "\n",
    "train_data=DatasetCreater(img_path,0,4000,\"png\",transformer_train)\n",
    "train_loader =DataLoader(\n",
    "\tdataset=train_data,\n",
    "    batch_size=batch_size0,\n",
    "    shuffle=True\n",
    "\t)\n",
    "test_data=DatasetCreater(img_path,4000,1000,\"png\",transformer_test)\n",
    "\n",
    "test_loader =DataLoader(\n",
    "\tdataset=test_data,\n",
    "    batch_size=batch_size0,\n",
    "    shuffle=True\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DatasetCreater_Testfunction\n",
    "#a.__getitem__(2) #male\n",
    "#a.__getitem__(4999) #female\n",
    "# Flag=0\n",
    "# for i,j in test_loader:\n",
    "# \tprint(i)\n",
    "# \tbreak\n",
    "    \n",
    "#什么是枚举函数呢？实际上enumerate函数自动调用了对象的__getitem__()方法，将其的每个输出对应一个序号，以index，value的形式进行枚举\n",
    "#无论是for还是枚举函数，什么时候停止迭代呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now ,I want to creat a neural network\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.convModule=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "             nn.Conv2d(\n",
    "                in_channels=feature_size,\n",
    "                out_channels=feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_size,\n",
    "                out_channels=2*feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(2*feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=2*feature_size,\n",
    "                out_channels=2*feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(2*feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # self.conv3=nn.Sequential(\n",
    "        #     nn.Conv2d(\n",
    "        #         in_channels=20,\n",
    "        #         out_channels=20,\n",
    "        #         kernel_size=3,\n",
    "        #         stride=1,\n",
    "        #         padding=1\n",
    "        #     ),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2)\n",
    "        #)\n",
    "        self.outLayer=nn.Sequential(\n",
    "            nn.Linear(80000,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,classesNum)\n",
    "            )\n",
    "         #10 feature maps\n",
    "        #this number 28512 is not good,I don't know how the outLayer number should be set\n",
    "        #So If you change the input size of the picture,this neural net work will raise a \n",
    "        #error which is \"mat1 and mat2 shapes cannot be multiplied (10x aNnumber and 28512x2)\"\"\n",
    "        #then,you should change 28512 to \"aNumber\"in there\n",
    "        #216/2/2=54 pooling twice，male&female，2output\n",
    "    \n",
    "    def forward(self,input):\n",
    "        input1=self.convModule(input)\n",
    "        input2=self.conv2(input1)\n",
    "        #input3=self.conv3(input2)\n",
    "        \n",
    "        input4=input2.view(input2.size(0),-1)\n",
    "        output=self.outLayer(input4)\n",
    "        return output\n",
    "        #we want to reshape the fature map into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision(predict_solution, labels):\n",
    "#     pred = torch.max(predict_solution.data, 1)[1] \n",
    "#     #torch.max()function return the biggest value of each row or column. if the \n",
    "#     # second argument is 0,group by column;if the second argument is 1,group by row\n",
    "#     # so torch.max(predict_solution, 1) return a \n",
    "#     rights = pred.eq(labels.data.view_as(pred)).sum() \n",
    "#     return rights, len(labels) \n",
    "def precision(predict_solution,labels):\n",
    "    pred = torch.max(predict_solution.data, 1)[1]\n",
    "    number_of_right=0\n",
    "    for label,value in enumerate(pred):\n",
    "        if value==labels[label]:\n",
    "            number_of_right+=1\n",
    "    return number_of_right/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_selection(Ir):\n",
    "        neuroNet=ConvolutionalNN()#.cuda()\n",
    "        #next,I  define a loss function and an optimizer\n",
    "        lossFunction=nn.CrossEntropyLoss()\n",
    "        #Cross Entropy loss is linked to mutiple classification problems，one-hot label\n",
    "        optimizer = optim.Adam(neuroNet.parameters(), Ir)\n",
    "        #Adam and Nadam are the best\n",
    "        #later,I want to choose the best Ir by model selection\n",
    "        flag=0#flag for training times\n",
    "        test_rights=[]\n",
    "        training_batch_times=[]\n",
    "        for i in range(epochs):\n",
    "                for bach,label in train_loader: \n",
    "                \n",
    "                        neuroNet.train()#set the test_loader_iter neural network to training model                        \n",
    "                        output = neuroNet(bach)#.cuda())\n",
    "                        loss = lossFunction(output,torch.tensor(label))#.cuda())\n",
    "\n",
    "                        flag+=1\n",
    "                        if flag%50==0:\n",
    "                                # neuroNet.eval()\n",
    "                                # print(\"training times=\",flag)dd\n",
    "                                #torch.cuda.empty_cache()\n",
    "                                test_batchs_number=0\n",
    "                                sum_of_precision=0\n",
    "                                for i,j in test_loader:\n",
    "                                        #sum_of_precision+=precision(neuroNet(i.cuda()).cpu(),j)\n",
    "                                        sum_of_precision+=precision(neuroNet(i),j)\n",
    "                                        test_batchs_number+=1\n",
    "                                test_right=sum_of_precision/test_batchs_number\n",
    "                                print(test_right)\n",
    "                                test_rights.append(test_right)\n",
    "                                training_batch_times.append(flag)\n",
    "                                if flag>=300:\n",
    "                                        if test_right==max(test_rights) and test_right>0.92:\n",
    "                                                return neuroNet,test_rights,training_batch_times\n",
    "                        optimizer.zero_grad()#clean the grade \n",
    "                        loss.backward()\n",
    "                        \n",
    "                        optimizer.step()\n",
    "        return neuroNet,test_rights,training_batch_times\n",
    "                # if flag%300==0:#\n",
    "                #         neuroNet.eval()#evaluate\n",
    "                #         train_right = precision(output, label)\n",
    "                #         train_rights.append(train_right)\n",
    "                #         test_data,test_label= next(test_loader__iter)\n",
    "                #         output = neuroNet(test_data) \n",
    "                #         test_right = precision(output, test_label) \n",
    "                #         test_rights.append(test_right)\n",
    "                # print(\"train_rights=\",train_rights[0],\"test_rights=\",test_rights[0])\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36161\\AppData\\Local\\Temp\\ipykernel_16908\\2713854667.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = lossFunction(output,torch.tensor(label))#.cuda())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8560000000000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#now,we want to select the best Ir and other parameters for our module\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# for Ir in [0.001,0.0012,0.0014,0.0016]:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m neuroNet,test_rights,training_batch_times\u001b[39m=\u001b[39mparameter_selection(\u001b[39m0.001\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[183], line 38\u001b[0m, in \u001b[0;36mparameter_selection\u001b[1;34m(Ir)\u001b[0m\n\u001b[0;32m     36\u001b[0m                                         \u001b[39mreturn\u001b[39;00m neuroNet,test_rights,training_batch_times\n\u001b[0;32m     37\u001b[0m                 optimizer\u001b[39m.\u001b[39mzero_grad()\u001b[39m#clean the grade \u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m                 loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     40\u001b[0m                 optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m neuroNet,test_rights,training_batch_times\n",
      "File \u001b[1;32mc:\\Users\\36161\\anaconda3\\envs\\MLAssignment\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\36161\\anaconda3\\envs\\MLAssignment\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#now,we want to select the best Ir and other parameters for our module\n",
    "# for Ir in [0.001,0.0012,0.0014,0.0016]:\n",
    "neuroNet,test_rights,training_batch_times=parameter_selection(0.001)\n",
    "#torch.save(neuroNet.state_dict(), \"./model_parameter.pkl\")\n",
    "\n",
    "# trained_model=ConvolutionalNN()\n",
    "# trained_model.load_state_dict(torch.load(\"./model_parameter.pkl\"))\n",
    "# test_batchs_number=0\n",
    "# sum_of_precision=0\n",
    "# for i,j in test_loader:\n",
    "#         sum_of_precision+=precision(trained_model(i),j)\n",
    "#         test_batchs_number+=1\n",
    "#         test_right=sum_of_precision/test_batchs_number\n",
    "# print(test_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_rights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAssignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12ff0241aab6d68d89df65f05969083ec5b85532f876719a35cf7715ba95e61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
