{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as function\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tookit import precision #this programe is used to strore commonly used functions\n",
    "if torch.cuda.is_available()==False:\n",
    "    raise ImportError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_width=178\n",
    "picture_height=218 #the width and height of picture in celeba\n",
    "#待会儿改成可以除以4的，比如54*4=216\n",
    "classesNum=2 #male or female\n",
    "epochs=2 #how much epoch I need to run\n",
    "batch_size0 =6 #I want to train it with minibatch.\n",
    "#there are 5000 images in celeba dataset\n",
    "img_path=os.getcwd()[:-2]+\"Datasets\\\\celeba\" #[:-2]for deleting \"A1\"\n",
    "feature_size=14\n",
    "#I will double the feature_size after every pooling layer.\n",
    "#so if you set feature_size to a very big number,It will become very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_loader(gloabel_path:str,label_index:int):\n",
    "    \"\"\" this class is for loading the label of picture.The label of picture is not well formated  \n",
    "    \"\"\"\n",
    "    new_label_path=gloabel_path+'\\\\labels.csv'\n",
    "    labels_original=pd.read_csv(new_label_path)  \n",
    "    labelsNew=[]\n",
    "    for row in labels_original.iterrows():\n",
    "    #self.labels is a Dataframe,we can only use .iterrows()method to iter a dataframe\n",
    "        splitedLabel=str(row).split('\\\\t')\n",
    "        labelsNew.append(int(splitedLabel[label_index]))\n",
    "    return labelsNew\n",
    "\n",
    "\n",
    "#this class is for loading data.\n",
    "class DatasetCreater(Dataset):\n",
    "    def __init__(self,path:str,start_item:int,data_size:int,data_format:str,transformer:object,labels):\n",
    "        #transformer is an object created by torchvision.transforms.Compose()to transform an picture from PIL object to tensor\n",
    "        super().__init__()\n",
    "        self.path=path\n",
    "        self.start_item=start_item\n",
    "        self.data_size=data_size\n",
    "        self.transformer=transformer\n",
    "        self.data_format=data_format\n",
    "        \n",
    "        #labels.Labels is small so I want to load every labes in the initial process\n",
    "        #pictures are big so I want to load it when I need to use it\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    #哈哈，枚举类型和for循环也不知道可迭代对象有几个元素。他们就是通过__len__()函数找要迭代多少次的。如果__len__函数大于\n",
    "    #真实可迭代次数，就会报错\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #the path of one data\n",
    "        OneDataPath=self.path+\"\\\\img\\\\\"+str(item+self.start_item)+\".\"+self.data_format\n",
    "        image=Image.open(OneDataPath).convert('RGB')\n",
    "        imageTensor=self.transformer(image)\n",
    "\n",
    "        label=self.labels[item+self.start_item]\n",
    "        return imageTensor,label\n",
    "\n",
    "transformer_train = transforms.Compose(\n",
    "    [\n",
    "    transforms.RandomHorizontalFlip(p=0.1),#随机水平翻转 选择一个概率概率 randon horizontal Flip\n",
    "    #Because there is no need to identify head-down photos in the dataset and the tasks we need to complete\n",
    "    #transforms.RandomRotation(30),#随机旋转，-45到45度之间随机选\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])         \n",
    "])\n",
    "\n",
    "transformer_test = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])         \n",
    "])\n",
    "\n",
    "\n",
    "labels=[x if x != -1 else 0 for x in label_loader(img_path,5)]\n",
    "#change -1 to 0\n",
    "train_data=DatasetCreater(img_path,0,4000,\"jpg\",transformer_train,labels)\n",
    "train_loader =DataLoader(\n",
    "\tdataset=train_data,\n",
    "    batch_size=batch_size0,\n",
    "    shuffle=True\n",
    "\t)\n",
    "test_data=DatasetCreater(img_path,4000,1000,\"jpg\",transformer_test,labels)\n",
    "\n",
    "test_loader =DataLoader(\n",
    "\tdataset=test_data,\n",
    "    batch_size=batch_size0,\n",
    "    shuffle=True\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DatasetCreater_Testfunction\n",
    "#a.__getitem__(2) #male\n",
    "#a.__getitem__(4999) #female\n",
    "# Flag=0\n",
    "# for i,j in test_loader:\n",
    "# \tprint(i)\n",
    "# \tbreak\n",
    "#什么是枚举函数呢？实际上enumerate函数自动调用了对象的__getitem__()方法，将其的每个输出对应一个序号，以index，value的形式进行枚举\n",
    "#无论是for还是枚举函数，什么时候停止迭代呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now ,I want to creat a neural network\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.convModule=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "             nn.Conv2d(\n",
    "                in_channels=feature_size,\n",
    "                out_channels=feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_size,\n",
    "                out_channels=2*feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(2*feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=2*feature_size,\n",
    "                out_channels=2*feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(2*feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.outLayer=nn.Sequential(\n",
    "            nn.Linear(66528,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,classesNum)\n",
    "            )\n",
    "        #10 feature maps\n",
    "        #this number 28512 is not good,I don't know how the outLayer number should be set\n",
    "        #So If you change the input size of the picture,this neural net work will raise a \n",
    "        #error which is \"mat1 and mat2 shapes cannot be multiplied (10x aNnumber and 28512x2)\"\"\n",
    "        #then,you should change 28512 to \"aNumber\"in there\n",
    "        #216/2/2=54 pooling twice，male&female，2output\n",
    "    \n",
    "    def forward(self,input):\n",
    "        input1=self.convModule(input)\n",
    "        input2=self.conv2(input1)\n",
    "        #input3=self.conv3(input2)\n",
    "        \n",
    "        input4=input2.view(input2.size(0),-1)\n",
    "        output=self.outLayer(input4)\n",
    "        return output\n",
    "        #we want to reshape the fature map into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_selection(Ir):\n",
    "        neuroNet=ConvolutionalNN().cuda()\n",
    "        #next,I  define a loss function and an optimizer\n",
    "        lossFunction=nn.CrossEntropyLoss()\n",
    "        #Cross Entropy loss is linked to mutiple classification problems，one-hot label\n",
    "        optimizer = optim.Adam(neuroNet.parameters(), Ir)\n",
    "        #Adam and Nadam are the best\n",
    "        #later,I want to choose the best Ir by model selection\n",
    "        flag=0#flag for training times\n",
    "        test_rights=[]\n",
    "        training_batch_times=[]\n",
    "        for i in range(epochs):\n",
    "                # flag=0\n",
    "                # #test_flag=0\n",
    "                # test_loader__iter=test_loader.__iter__()\n",
    "                #an iter of test_loader\n",
    "                \n",
    "                for bach,label in train_loader: \n",
    "                        print(bach,label)\n",
    "                        neuroNet.train()#set the test_loader_iter neural network to training model                        \n",
    "                        output = neuroNet(bach.cuda())\n",
    "                        loss = lossFunction(output,label.cuda())\n",
    "\n",
    "                        flag+=1\n",
    "                        if flag%50==0:\n",
    "                                # neuroNet.eval()\n",
    "                                # print(\"training times=\",flag)\n",
    "                                torch.cuda.empty_cache()\n",
    "                                test_batchs_number=0\n",
    "                                sum_of_precision=0\n",
    "                                for i,j in test_loader:\n",
    "                                        sum_of_precision+=precision(neuroNet(i.cuda()).cpu(),j)\n",
    "                                        test_batchs_number+=1\n",
    "                                test_right=sum_of_precision/test_batchs_number\n",
    "                                print(test_right)\n",
    "                                test_rights.append(test_right)\n",
    "                                training_batch_times.append(flag)\n",
    "                                if flag>=300:\n",
    "                                        if test_right==max(test_rights) and test_right>0.92:\n",
    "                                                return neuroNet,test_rights,training_batch_times\n",
    "                        optimizer.zero_grad()#clean the grade \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "        return neuroNet,test_rights,training_batch_times\n",
    "                # if flag%300==0:#\n",
    "                #         neuroNet.eval()#evaluate\n",
    "                #         train_right = precision(output, label)\n",
    "                #         train_rights.append(train_right)\n",
    "                #         test_data,test_label= next(test_loader__iter)\n",
    "                #         output = neuroNet(test_data) \n",
    "                #         test_right = precision(output, test_label) \n",
    "                #         test_rights.append(test_right)\n",
    "                # print(\"train_rights=\",train_rights[0],\"test_rights=\",test_rights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvolutionalNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#now,we want to select the best Ir and other parameters for our module\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# for Ir in [0.001,0.0012,0.0014,0.0016]:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#    neuroNet,test_rights,training_batch_times=parameter_selection(0.001)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#torch.save(neuroNet.state_dict(), \"./model_parameter.pkl\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m trained_model\u001b[39m=\u001b[39mConvolutionalNN()\n\u001b[0;32m      7\u001b[0m trained_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./model_parameter.pkl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m test_batchs_number\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConvolutionalNN' is not defined"
     ]
    }
   ],
   "source": [
    "#now,we want to select the best Ir and other parameters for our module\n",
    "# for Ir in [0.001,0.0012,0.0014,0.0016]:\n",
    "#    neuroNet,test_rights,training_batch_times=parameter_selection(0.001)\n",
    "#torch.save(neuroNet.state_dict(), \"./model_parameter.pkl\")\n",
    "\n",
    "trained_model=ConvolutionalNN()\n",
    "trained_model.load_state_dict(torch.load(\"./model_parameter.pkl\"))\n",
    "test_batchs_number=0\n",
    "sum_of_precision=0\n",
    "for i,j in test_loader:\n",
    "        sum_of_precision+=precision(trained_model(i),j)\n",
    "        test_batchs_number+=1\n",
    "        test_right=sum_of_precision/test_batchs_number\n",
    "print(test_right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAssignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12ff0241aab6d68d89df65f05969083ec5b85532f876719a35cf7715ba95e61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
