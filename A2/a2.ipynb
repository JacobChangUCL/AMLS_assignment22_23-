{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as function\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available()==False:\n",
    "    raise ImportError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_width=178\n",
    "picture_height=218 #the width and height of picture in celeba\n",
    "#待会儿改成可以除以4的，比如54*4=216\n",
    "classesNum=2 #male or female\n",
    "epochs=3 #how much epoch I need to run\n",
    "batch_size0 =6 #I want to train it with minibatch.\n",
    "#there are 5000 images in celeba dataset\n",
    "img_path=os.getcwd()[:-2]+\"Datasets\\\\celeba\" #[:-2]for delet \"A1\"\n",
    "\n",
    "feature_size=14\n",
    "#I will double the feature_size after every pooling layer.so if you set feature_size to a very big number,\n",
    "#It will become very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class is for loading data.\n",
    "class DatasetCreater(Dataset):\n",
    "    def __init__(self,path:str,start_item:int,data_size:int,data_format:str,transformer:object):\n",
    "        #transformer is an object created by torchvision.transforms.Compose()to transform an picture from PIL object to tensor\n",
    "        super().__init__()\n",
    "        self.path=path\n",
    "        self.start_item=start_item\n",
    "        self.data_size=data_size\n",
    "        self.transformer=transformer\n",
    "        self.data_format=data_format\n",
    "            \n",
    "        #we process labels.Labels are small so I want to load every labes in the initial process\n",
    "        #pictures are big so I want to load it when I need to use it\n",
    "        labelPath=self.path+'\\\\labels.csv'\n",
    "        self.labels=pd.read_csv(labelPath)\n",
    "        self.labelsNew=[]\n",
    "        \n",
    "        for row in self.labels.iterrows():\n",
    "        #self.labels is a Dataframe,we can only use .iterrows()method to iter a dataframe\n",
    "            Name_index = str(row).find(\"Name\")\n",
    "            self.labelsNew.append(0 if str(row)[Name_index-3]==\"-\" else 1)\n",
    "       # we change ”-1“ to 0 and change ”1“ to 1 ,0 represents \"not smiling\" and 1 repersents \"smiling\"\n",
    "        #现在我有一个imgs的数组，这个数组里面每一个元素都是一个图片tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    #哈哈，枚举类型和for循环也不知道可迭代对象有几个元素。他们就是通过__len__()函数找要迭代多少次的。如果__len__函数大于\n",
    "    #真实可迭代次数，就会报错\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #the path of one data\n",
    "        OneDataPath=self.path+\"\\\\img\\\\\"+str(item+self.start_item)+\".\"+self.data_format\n",
    "        image=Image.open(OneDataPath).convert('RGB')\n",
    "        imageTensor=self.transformer(image)\n",
    "\n",
    "        label=self.labelsNew[item+self.start_item]\n",
    "        return imageTensor,label\n",
    "\n",
    "transformer_train = transforms.Compose(\n",
    "    [\n",
    "   # transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.1),#随机水平翻转 选择一个概率概率 randon horizontal Flip\n",
    "    # transforms.RandomVerticalFlip(p=0.2),#随机垂直翻转 I have tested this function,the performance is not good.\n",
    "    #Because there is no need to identify head-down photos in the dataset and the tasks we need to complete\n",
    "    #transforms.RandomRotation(30),#随机旋转，-45到45度之间随机选\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])         \n",
    "])\n",
    "\n",
    "transformer_test = transforms.Compose(\n",
    "    [\n",
    "   # transforms.Resize((256,256)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.2),#随机水平翻转 选择一个概率概率 randon horizontal Flip\n",
    "    # transforms.RandomVerticalFlip(p=0.2),#随机垂直翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])         \n",
    "])\n",
    "\n",
    "train_data=DatasetCreater(img_path,0,4000,\"jpg\",transformer_train)\n",
    "train_loader =DataLoader(\n",
    "\tdataset=train_data,\n",
    "    batch_size=batch_size0,\n",
    "    shuffle=True\n",
    "\t)\n",
    "test_data=DatasetCreater(img_path,4000,1000,\"jpg\",transformer_test)\n",
    "\n",
    "test_loader =DataLoader(\n",
    "\tdataset=test_data,\n",
    "    batch_size=batch_size0,\n",
    "    shuffle=True\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DatasetCreater_Testfunction\n",
    "#a.__getitem__(2) #male\n",
    "#a.__getitem__(4999) #female\n",
    "# Flag=0\n",
    "# for i,j in test_loader:\n",
    "# \tprint(i)\n",
    "# \tbreak\n",
    "    \n",
    "#什么是枚举函数呢？实际上enumerate函数自动调用了对象的__getitem__()方法，将其的每个输出对应一个序号，以index，value的形式进行枚举\n",
    "#无论是for还是枚举函数，什么时候停止迭代呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now ,I want to creat a neural network\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.convModule=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            #nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "             nn.Conv2d(\n",
    "                in_channels=feature_size,\n",
    "                out_channels=feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_size,\n",
    "                out_channels=2*feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            #nn.BatchNorm2d(2*feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=2*feature_size,\n",
    "                out_channels=2*feature_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(2*feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # self.conv3=nn.Sequential(\n",
    "        #     nn.Conv2d(\n",
    "        #         in_channels=20,\n",
    "        #         out_channels=20,\n",
    "        #         kernel_size=3,\n",
    "        #         stride=1,\n",
    "        #         padding=1\n",
    "        #     ),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2)\n",
    "        #)\n",
    "        self.outLayer=nn.Sequential(\n",
    "            nn.Linear(66528,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,2)\n",
    "            )\n",
    "         #10 feature maps\n",
    "        #this number 28512 is not good,I don't know how the outLayer number should be set\n",
    "        #So If you change the input size of the picture,this neural net work will raise a \n",
    "        #error which is \"mat1 and mat2 shapes cannot be multiplied (10x aNnumber and 28512x2)\"\"\n",
    "        #then,you should change 28512 to \"aNumber\"in there\n",
    "        #216/2/2=54 pooling twice，male&female，2output\n",
    "    \n",
    "    def forward(self,input):\n",
    "        input1=self.convModule(input)\n",
    "        input2=self.conv2(input1)\n",
    "        #input3=self.conv3(input2)\n",
    "        \n",
    "        input4=input2.view(input2.size(0),-1)\n",
    "        output=self.outLayer(input4)\n",
    "        return output\n",
    "        #we want to reshape the fature map into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision(predict_solution, labels):\n",
    "#     pred = torch.max(predict_solution.data, 1)[1] \n",
    "#     #torch.max()function return the biggest value of each row or column. if the \n",
    "#     # second argument is 0,group by column;if the second argument is 1,group by row\n",
    "#     # so torch.max(predict_solution, 1) return a \n",
    "#     rights = pred.eq(labels.data.view_as(pred)).sum() \n",
    "#     return rights, len(labels) \n",
    "def precision(predict_solution,labels):\n",
    "    pred = torch.max(predict_solution.data, 1)[1]\n",
    "    number_of_right=0\n",
    "    for label,value in enumerate(pred):\n",
    "        if value==labels[label]:\n",
    "            number_of_right+=1\n",
    "    return number_of_right/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_selection(Ir):\n",
    "        neuroNet=ConvolutionalNN().cuda()\n",
    "        #next,I  define a loss function and an optimizer\n",
    "        lossFunction=nn.CrossEntropyLoss()\n",
    "        #Cross Entropy loss is linked to mutiple classification problems，one-hot label\n",
    "        optimizer = optim.Adam(neuroNet.parameters(), Ir)\n",
    "        #Adam and Nadam are the best\n",
    "        #later,I want to choose the best Ir by model selection\n",
    "        flag=0#flag for training times\n",
    "        test_rights=[]\n",
    "        training_batch_times=[]\n",
    "        for i in range(epochs):\n",
    "                # flag=0\n",
    "                # #test_flag=0\n",
    "                # test_loader__iter=test_loader.__iter__()\n",
    "                #an iter of test_loader\n",
    "                \n",
    "                for bach,label in train_loader: \n",
    "                        \n",
    "                        \n",
    "                        neuroNet.train()#set the test_loader_iter neural network to training model                        \n",
    "                        output = neuroNet(bach.cuda())\n",
    "                        loss = lossFunction(output,label.cuda())\n",
    "\n",
    "                        flag+=1\n",
    "                        if flag%50==0:\n",
    "                                # neuroNet.eval()\n",
    "                                # print(\"training times=\",flag)\n",
    "                                torch.cuda.empty_cache()\n",
    "                                test_batchs_number=0\n",
    "                                sum_of_precision=0\n",
    "                                for i,j in test_loader:\n",
    "                                        sum_of_precision+=precision(neuroNet(i.cuda()).cpu(),j)\n",
    "                                        test_batchs_number+=1\n",
    "                                test_right=sum_of_precision/test_batchs_number\n",
    "                                print(test_right)\n",
    "                                test_rights.append(test_right)\n",
    "                                training_batch_times.append(flag)\n",
    "                                if flag>=500:\n",
    "                                        if test_right==max(test_rights) and test_right>0.85:\n",
    "                                                return neuroNet,test_rights,training_batch_times\n",
    "                        optimizer.zero_grad()#clean the grade \n",
    "                        loss.backward()\n",
    "                        \n",
    "                        optimizer.step()\n",
    "        return neuroNet,test_rights,training_batch_times\n",
    "                # if flag%300==0:#\n",
    "                #         neuroNet.eval()#evaluate\n",
    "                #         train_right = precision(output, label)\n",
    "                #         train_rights.append(train_right)\n",
    "                #         test_data,test_label= next(test_loader__iter)\n",
    "                #         output = neuroNet(test_data) \n",
    "                #         test_right = precision(output, test_label) \n",
    "                #         test_rights.append(test_right)\n",
    "                # print(\"train_rights=\",train_rights[0],\"test_rights=\",test_rights[0])\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now,we want to select the best Ir and other parameters for our module\n",
    "# # for Ir in [0.001,0.0012,0.0014,0.0016]:\n",
    "# neuroNet,test_rights,training_batch_times=parameter_selection(0.0013)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(training_batch_times, test_rights,c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neuroNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39msave(neuroNet\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39m./model_parameter.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m trained_model\u001b[39m=\u001b[39mConvolutionalNN()\n\u001b[0;32m      4\u001b[0m trained_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./model_parameter.pkl\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'neuroNet' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(neuroNet.state_dict(), \"./model_parameter.pkl\")\n",
    "\n",
    "trained_model=ConvolutionalNN()\n",
    "trained_model.load_state_dict(torch.load(\"./model_parameter.pkl\"))\n",
    "test_batchs_number=0\n",
    "sum_of_precision=0\n",
    "for i,j in test_loader:\n",
    "        sum_of_precision+=precision(trained_model(i),j)\n",
    "        test_batchs_number+=1\n",
    "        test_right=sum_of_precision/test_batchs_number\n",
    "print(test_right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAssignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12ff0241aab6d68d89df65f05969083ec5b85532f876719a35cf7715ba95e61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
